{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpfhttp://localhost:8890/notebooks/m-3/Classification-Final-release-repeated.ipynb#ul analytics libraries installed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense,LSTM,GRU,Dropout,Masking,TimeDistributed,RepeatVector,GRU,SimpleRNN,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = \"alive\"\n",
    "dbname = \"MIMIC-III\"\n",
    "hr_data = 48\n",
    "hr_lab = int(48/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vital = ['diastolicbp', 'systolicbp',\n",
    "       'heartrate', 'meanbp', 'temperature', 'spo2', 'respiration',]\n",
    "\n",
    "col_lab = [\n",
    "       'bicarbonate', 'bun', 'chloride', 'creatinine', 'glucose', 'hemoglobin',\n",
    "       'hematocrit', 'lactate', 'platelet', 'potassium', 'sodium', 'wbc',\n",
    "       'bilirubin', 'albumin', 'bands', 'ptt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_vital = pd.read_pickle('db_vital48-no-scale.pickle')\n",
    "db_lab = pd.read_pickle('db_lab48-no-scale.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(X, Y):\n",
    "    num_pos = np.sum(Y==1)\n",
    "    num_neg = np.sum(Y==0)\n",
    "    #print(f'#Pos: {num_pos} #Neg: {num_neg} >> ', end='')\n",
    "\n",
    "    idxpos = np.where(Y==1)[0]\n",
    "    idxneg = np.where(Y==0)[0]\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "    choices = rng.choice(idxpos, num_neg)\n",
    "    #print(f'#Pos: {len(choices)} #Neg: {num_neg}')\n",
    "\n",
    "    RX = np.concatenate([X[choices], X[idxneg]], axis=0)\n",
    "    RY = np.concatenate([Y[choices], Y[idxneg]], axis=0)\n",
    "    \n",
    "    return RX, RY\n",
    "\n",
    "def undersample(X, Y):\n",
    "    num_pos = np.sum(Y==1)\n",
    "    num_neg = np.sum(Y==0)\n",
    "    #print(f'#Pos: {num_pos} #Neg: {num_neg} >> ', end='')\n",
    "\n",
    "    idxpos = np.where(Y==1)[0]\n",
    "    idxneg = np.where(Y==0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(1)\n",
    "    choices = rng.choice(idxneg, num_pos)\n",
    "    #print(f'#Pos: {num_pos} #Neg: {len(choices)}')\n",
    "\n",
    "    RX = np.concatenate([X[idxpos], X[choices]], axis=0)\n",
    "    RY = np.concatenate([Y[idxpos], Y[choices]], axis=0)\n",
    "    \n",
    "    return RX, RY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df, cols, hrs, ktrainid, kvalid, ktestid, mode='oversample', baseline=False):\n",
    "    values = df.values\n",
    "\n",
    "    values_train = values[ktrainid]\n",
    "    values_validation = values[kvalid]\n",
    "    values_test = values[ktestid]\n",
    "\n",
    "    trainX, trainY = values_train[:,:-2], values_train[:,-2]\n",
    "    valX, valY = values_validation[:,:-2], values_validation[:,-2]\n",
    "    testX, testY = values_test[:,:-2], values_test[:,-2]\n",
    "\n",
    "    if baseline == False:\n",
    "        trainX = trainX.reshape(trainX.shape[0],hrs,len(cols))\n",
    "        valX = valX.reshape(valX.shape[0],hrs,len(cols))\n",
    "        testX = testX.reshape(testX.shape[0],hrs,len(cols))\n",
    "    \n",
    "    #print(f'TrainX: {str(trainX.shape)} TrainY: {str(trainY.shape)}  [{np.sum(trainY==0)}+{np.sum(trainY==1)}]')\n",
    "    #print(f'ValidX: {str(valX.shape)}  ValidX: {str(valY.shape)}   [{np.sum(valY==0)}+{np.sum(valY==1)}]')\n",
    "    #print(f'TestX:  {str(testX.shape)}  TestX:  {str(testY.shape)}   [{np.sum(testY==0)}+{np.sum(testY==1)}]')\n",
    "\n",
    "\n",
    "    if mode == 'oversample':\n",
    "        trainX, trainY = oversample(trainX, trainY)\n",
    "        valX, valY = oversample(valX, valY)\n",
    "        testX, testY = oversample(testX, testY)\n",
    "    elif mode == 'undersample':\n",
    "        trainX, trainY = undersample(trainX, trainY)\n",
    "        valX, valY = undersample(valX, valY)\n",
    "        testX, testY = undersample(testX, testY)\n",
    "    \n",
    "    return (trainX, trainY), (valX, valY), (testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        #print(\"\\nEpoch %05d: Learning rate is %f.\" % (epoch, scheduled_lr))\n",
    "    \n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    m = 1\n",
    "    if( epoch % m == m or epoch % m == 0):\n",
    "        drop = 0.055\n",
    "        epochs_drop = 2.0\n",
    "        lrate = lr * 1/(1 + drop * epoch)\n",
    "    else:\n",
    "        lrate = lr\n",
    "\n",
    "    return lrate\n",
    "\n",
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        yhat = model.predict(testX, verbose=0)\n",
    "        diff = value_restore(yhat, train_mean, train_std) -  value_restore(testy, train_mean, train_std)\n",
    "        for i, col in enumerate(column_indices):\n",
    "            print(\"{} > mae = {:f}\".format(col, np.abs(diff[:,:,i]).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(trainXY, valXY, testXY, hrs, k, arch, name, verbose=1):\n",
    "\n",
    "    print(f'MODEL {name.upper()}', end='\\t')\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_shape=[trainXY[0].shape[1]],activation = 'relu', units=64))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=64))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=64))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "    architecture = '{}-vital-{}-k{}'.format(arch, hrs, k)\n",
    "    logdir = os.path.join(\"logs-class\",\"{}-{}\".format(architecture,\n",
    "                                                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                            ,CustomLearningRateScheduler(lr_schedule)\n",
    "                            ,EarlyStopping(monitor='val_loss', patience=10)\n",
    "                            ,ModelCheckpoint(filepath=\"./{}/{}-best-{}-hr.h5\".format(logdir,architecture,hrs)\n",
    "                            ,monitor='val_loss'\n",
    "                            ,save_best_only=True)]\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.optimizers.Adam(0.01, beta_1=0.1, beta_2=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model.fit(trainXY[0], trainXY[1], \n",
    "                        validation_data=valXY,\n",
    "                        shuffle=True, epochs=20, verbose=verbose,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n",
    "        validYpred = model.predict(valXY[0])\n",
    "        testYpred = model.predict(testXY[0])\n",
    "        \n",
    "        # OPTIMAL THRESHOLD\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(valXY[1], validYpred, pos_label=1)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # PERFORMANCE\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(testXY[1], testYpred, pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tn, fp, fn, tp  = sklearn.metrics.confusion_matrix(testXY[1], testYpred>optimal_threshold).ravel()\n",
    "        macc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        msen = tp/(tp+fn)\n",
    "        mspc = tn/(tn+fp)\n",
    "        print(f'AUC: {auc:.4f}\\tAccuracy: {macc:.4f}\\tSensitivity: {msen:.4f}\\tSpecificity {mspc:.4f}')\n",
    "    \n",
    "    return model, (testXY[1], testYpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(trainXY, valXY, testXY, hrs, k, arch, name, verbose=1):\n",
    "    print(f'MODEL {name.upper()}', end='\\t')\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(input_shape=(trainXY[0].shape[1], trainXY[0].shape[2]), units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(SimpleRNN(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(SimpleRNN(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=False, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    architecture = '{}-vital-{}-k{}'.format(arch, hrs, k)\n",
    "    logdir = os.path.join(\"logs-class\",\"{}-{}\".format(architecture,\n",
    "                                                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                            ,CustomLearningRateScheduler(lr_schedule)\n",
    "                            ,EarlyStopping(monitor='val_loss', patience=10)\n",
    "                            ,ModelCheckpoint(filepath=\"./{}/{}-best-{}-hr.h5\".format(logdir,architecture,hrs)\n",
    "                            ,monitor='val_loss'\n",
    "                            ,save_best_only=True)]\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.optimizers.Adam(0.005, beta_1=0.1, beta_2=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model.fit(trainXY[0], trainXY[1], \n",
    "                        validation_data=valXY,\n",
    "                        shuffle=True, epochs=20, verbose=verbose,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n",
    "        validYpred = model.predict(valXY[0])\n",
    "        testYpred = model.predict(testXY[0])\n",
    "        \n",
    "        # OPTIMAL THRESHOLD\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(valXY[1], validYpred, pos_label=1)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # PERFORMANCE\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(testXY[1], testYpred, pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tn, fp, fn, tp  = sklearn.metrics.confusion_matrix(testXY[1], testYpred>optimal_threshold).ravel()\n",
    "        macc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        msen = tp/(tp+fn)\n",
    "        mspc = tn/(tn+fp)\n",
    "        print(f'AUC: {auc:.4f}\\tAccuracy: {macc:.4f}\\tSensitivity: {msen:.4f}\\tSpecificity {mspc:.4f}')\n",
    "    \n",
    "    return model, (testXY[1], testYpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(trainXY, valXY, testXY, hrs, k, arch, name, verbose=1):\n",
    "    print(f'MODEL {name.upper()}', end='\\t')\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(input_shape=(trainXY[0].shape[1], trainXY[0].shape[2]), units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LSTM(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LSTM(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=False, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    architecture = '{}-vital-{}-k{}'.format(arch, hrs, k)\n",
    "    logdir = os.path.join(\"logs-class\",\"{}-{}\".format(architecture,\n",
    "                                                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                            ,CustomLearningRateScheduler(lr_schedule)\n",
    "                            ,EarlyStopping(monitor='val_loss', patience=10)\n",
    "                            ,ModelCheckpoint(filepath=\"./{}/{}-best-{}-hr.h5\".format(logdir,architecture,hrs)\n",
    "                            ,monitor='val_loss'\n",
    "                            ,save_best_only=True)]\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.optimizers.Adam(0.005, beta_1=0.1, beta_2=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model.fit(trainXY[0], trainXY[1], \n",
    "                        validation_data=valXY,\n",
    "                        shuffle=True, epochs=20, verbose=verbose,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n",
    "        validYpred = model.predict(valXY[0])\n",
    "        testYpred = model.predict(testXY[0])\n",
    "        \n",
    "        # OPTIMAL THRESHOLD\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(valXY[1], validYpred, pos_label=1)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # PERFORMANCE\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(testXY[1], testYpred, pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tn, fp, fn, tp  = sklearn.metrics.confusion_matrix(testXY[1], testYpred>optimal_threshold).ravel()\n",
    "        macc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        msen = tp/(tp+fn)\n",
    "        mspc = tn/(tn+fp)\n",
    "        print(f'AUC: {auc:.4f}\\tAccuracy: {macc:.4f}\\tSensitivity: {msen:.4f}\\tSpecificity {mspc:.4f}')\n",
    "    \n",
    "    return model, (testXY[1], testYpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_model(trainXY, valXY, testXY, hrs, k, arch, name, verbose=1):\n",
    "    print(f'MODEL {name.upper()}', end='\\t')\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(input_shape=(trainXY[0].shape[1], trainXY[0].shape[2]), units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GRU(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=True, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GRU(units=16,\n",
    "                       dropout=0.0, recurrent_dropout=0.0,\n",
    "                       return_sequences=False, return_state=False,\n",
    "                       stateful=False, unroll=False\n",
    "                      ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    architecture = '{}-vital-{}-k{}'.format(arch, hrs, k)\n",
    "    logdir = os.path.join(\"logs-class\",\"{}-{}\".format(architecture,\n",
    "                                                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                            ,CustomLearningRateScheduler(lr_schedule)\n",
    "                            ,EarlyStopping(monitor='val_loss', patience=10)\n",
    "                            ,ModelCheckpoint(filepath=\"./{}/{}-best-{}-hr.h5\".format(logdir,architecture,hrs)\n",
    "                            ,monitor='val_loss'\n",
    "                            ,save_best_only=True)]\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.optimizers.Adam(0.005, beta_1=0.1, beta_2=0.001, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model.fit(trainXY[0], trainXY[1], \n",
    "                        validation_data=valXY,\n",
    "                        shuffle=True, epochs=20, verbose=verbose,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "\n",
    "        validYpred = model.predict(valXY[0])\n",
    "        testYpred = model.predict(testXY[0])\n",
    "        \n",
    "        # OPTIMAL THRESHOLD\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(valXY[1], validYpred, pos_label=1)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # PERFORMANCE\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(testXY[1], testYpred, pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tn, fp, fn, tp  = sklearn.metrics.confusion_matrix(testXY[1], testYpred>optimal_threshold).ravel()\n",
    "        macc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        msen = tp/(tp+fn)\n",
    "        mspc = tn/(tn+fp)\n",
    "        print(f'AUC: {auc:.4f}\\tAccuracy: {macc:.4f}\\tSensitivity: {msen:.4f}\\tSpecificity {mspc:.4f}')\n",
    "    \n",
    "    return model, (testXY[1], testYpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(v_trainXY, v_valXY, v_testXY,\n",
    "                   l_trainXY, l_valXY, l_testXY,\n",
    "                   hrs, k, arch, verbose=1):\n",
    "\n",
    "    if arch == 'DNN':\n",
    "        model_vital, results_vital = baseline_model(v_trainXY, v_valXY, v_testXY, hrs, k, arch, 'vital', verbose)\n",
    "        model_lab, results_lab = baseline_model(l_trainXY, l_valXY, l_testXY, hrs/8, k, arch, 'lab', verbose)\n",
    "    elif arch == 'RNN':\n",
    "        model_vital, results_vital = rnn_model(v_trainXY, v_valXY, v_testXY, hrs, k, arch, 'vital', verbose)\n",
    "        model_lab, results_lab = rnn_model(l_trainXY, l_valXY, l_testXY, hrs/8, k, arch, 'lab', verbose)\n",
    "    elif arch == 'LSTM':\n",
    "        model_vital, results_vital = lstm_model(v_trainXY, v_valXY, v_testXY, hrs, k, arch, 'vital', verbose)\n",
    "        model_lab, results_lab = lstm_model(l_trainXY, l_valXY, l_testXY, hrs/8, k, arch, 'lab', verbose)\n",
    "    elif arch == 'GRU':\n",
    "        model_vital, results_vital = gru_model(v_trainXY, v_valXY, v_testXY, hrs, k, arch, 'vital', verbose)\n",
    "        model_lab, results_lab = gru_model(l_trainXY, l_valXY, l_testXY, hrs/8, k, arch, 'lab', verbose)\n",
    "        \n",
    "    print(f'MODEL CONCAT', end='\\t')\n",
    "\n",
    "    # Combine\n",
    "    concatenated = tf.keras.layers.concatenate([model_vital.layers[-2].output, model_lab.layers[-2].output])\n",
    "    if arch == 'DNN':\n",
    "        out = Dense(units=64)(concatenated)\n",
    "    else:\n",
    "        out = Dense(units=8)(concatenated)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dense(1, activation='sigmoid', name='output_layer')(out)\n",
    "\n",
    "    model_concat = tf.keras.Model([model_vital.input, model_lab.input], out)\n",
    "    if verbose == 1:\n",
    "        model_concat.summary()\n",
    "\n",
    "    architecture = '{}-vital-lab-{}'.format(arch, hrs)\n",
    "    logdir = os.path.join(\"logs-class\",\"{}-{}\".format(architecture,\n",
    "                                                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                            ,CustomLearningRateScheduler(lr_schedule)\n",
    "                            ,EarlyStopping(monitor='val_loss', patience=5)\n",
    "                            ,ModelCheckpoint(filepath=\"./{}/{}-best-{}-hr.h5\".format(logdir,architecture,hr_data)\n",
    "                            ,monitor='val_loss'\n",
    "                            ,save_best_only=True)]\n",
    "\n",
    "    model_concat.compile(loss='binary_crossentropy', \n",
    "                         optimizer=tf.optimizers.Adam(0.01, beta_1=0.1, beta_2=0.001, amsgrad=True), \n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        model_concat.fit(x=[v_trainXY[0],l_trainXY[0]]\n",
    "                        ,y=v_trainXY[1]\n",
    "                        ,shuffle=True\n",
    "                        ,epochs=20\n",
    "                        ,verbose=verbose\n",
    "                        ,validation_data=([v_valXY[0],l_valXY[0]], v_valXY[1])\n",
    "                        ,callbacks=[tensorboard_callback])\n",
    "\n",
    "        validYpred = model_concat.predict([v_valXY[0],l_valXY[0]])\n",
    "        testYpred = model_concat.predict([v_testXY[0],l_testXY[0]])\n",
    "        \n",
    "        # OPTIMAL THRESHOLD\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(v_valXY[1], validYpred, pos_label=1)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # PERFORMANCE\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(v_testXY[1], testYpred, pos_label=1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tn, fp, fn, tp  = sklearn.metrics.confusion_matrix(v_testXY[1], testYpred>optimal_threshold).ravel()\n",
    "        macc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        msen = tp/(tp+fn)\n",
    "        mspc = tn/(tn+fp)\n",
    "        print(f'AUC: {auc:.4f}\\tAccuracy: {macc:.4f}\\tSensitivity: {msen:.4f}\\tSpecificity {mspc:.4f}')\n",
    "\n",
    "    results_concat = (v_testXY[1], testYpred)\n",
    "    \n",
    "    return model_vital, model_lab, model_concat, results_vital, results_lab, results_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(modelv, modell, modelc, datav, datal):\n",
    "    pert_v = []\n",
    "    pert_l = []\n",
    "    pert_c = []\n",
    "    print('Pertubation: Vitals')\n",
    "    orig_out = modelv.predict(datav)\n",
    "    for i in range(datav.shape[2]):\n",
    "        new_x = datav.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = modelv.predict(new_x)\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n",
    "        print(f'Variable [{col_vital[i]: <16}], perturbation effect: {effect:.4f}')\n",
    "        pert_v.append(effect)\n",
    "\n",
    "    print('Pertubation: Labs')\n",
    "    orig_out = modell.predict(datal)\n",
    "    for i in range(datal.shape[2]):\n",
    "        new_x = datal.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = modell.predict(new_x)\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n",
    "        print(f'Variable [{col_lab[i]: <16}], perturbation effect: {effect:.4f}')\n",
    "        pert_l.append(effect)\n",
    "    \n",
    "    print('Pertubation: All')\n",
    "    orig_out = mc.predict([datav, datal])\n",
    "    for i in range(datav.shape[2]):\n",
    "        new_x = datav.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = mc.predict([new_x, datal])\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n",
    "        print(f'Variable [{col_vital[i]: <16}], perturbation effect: {effect:.4f}')\n",
    "        pert_c.append(effect)\n",
    "\n",
    "    for i in range(datal.shape[2]):\n",
    "        new_x = datal.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = mc.predict([datav,new_x])\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n",
    "        print(f'Variable [{col_lab[i]: <16}], perturbation effect: {effect:.4f}')\n",
    "        pert_c.append(effect)\n",
    "        \n",
    "    pert_v = zip(col_vital, pert_v)\n",
    "    pert_l = zip(col_lab, pert_l)\n",
    "    pert_c = zip(col_vital + col_lab, pert_c)\n",
    "    return pert_v, pert_l, pert_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "num_k = 5\n",
    "num_sample = db_lab.shape[0]\n",
    "train_idx, test_idx = train_test_split(range(num_sample), test_size=0.15, random_state=0)\n",
    "\n",
    "kf = KFold(n_splits=num_k, shuffle=True, random_state=0)\n",
    "ktrain_idxes = []\n",
    "kval_idxes = []\n",
    "train_idx = np.array(train_idx)\n",
    "\n",
    "for ktrain_idx, kval_idx in kf.split(train_idx):\n",
    "    ktrain_idxes.append(train_idx[ktrain_idx])\n",
    "    kval_idxes.append(train_idx[kval_idx])\n",
    "    \n",
    "def flat(a):\n",
    "    a1 = np.reshape(a[0], (a[0].shape[0], -1))\n",
    "    return (a1, a[1])\n",
    "\n",
    "results_baseline = []\n",
    "for ki in range(num_k):\n",
    "    print(f'--------- k = {ki} ---------')\n",
    "    VtrainXY, VvalXY, VtestXY = data_prep(db_vital, col_vital, hr_data,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=True)\n",
    "    LtrainXY, LvalXY, LtestXY = data_prep(db_lab, col_lab, hr_lab,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=True)\n",
    "    mv, ml, mc, rv, rl, rc = train_val_test(flat(VtrainXY),flat(VvalXY),\n",
    "                                            flat(VtestXY),flat(LtrainXY),\n",
    "                                            flat(LvalXY),flat(LtestXY)\n",
    "                                            ,48,0,'DNN',0)\n",
    "    results_baseline.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'data': [results_baseline]}).to_pickle('results_baseline{}.pickle'.format(hr_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN\n",
    "num_k = 5\n",
    "num_sample = db_lab.shape[0]\n",
    "train_idx, test_idx = train_test_split(range(num_sample), test_size=0.15, random_state=0)\n",
    "\n",
    "kf = KFold(n_splits=num_k, shuffle=True, random_state=0)\n",
    "train_idx = np.array(train_idx)\n",
    "ktrain_idxes = []\n",
    "kval_idxes = []\n",
    "for ktrain_idx, kval_idx in kf.split(train_idx):\n",
    "    ktrain_idxes.append(train_idx[ktrain_idx])\n",
    "    kval_idxes.append(train_idx[kval_idx])\n",
    "    \n",
    "results_rnn = []\n",
    "for ki in range(num_k):\n",
    "    print(f'--------- k = {ki} ---------')\n",
    "    VtrainXY, VvalXY, VtestXY = data_prep(db_vital, col_vital, hr_data,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    LtrainXY, LvalXY, LtestXY = data_prep(db_lab, col_lab, hr_lab,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    mv, ml, mc, rv, rl, rc = train_val_test(VtrainXY,VvalXY,VtestXY,LtrainXY,LvalXY,LtestXY,48,0,'RNN',0)\n",
    "    results_rnn.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'data': [results_rnn]}).to_pickle('results_rnn{}.pickle'.format(hr_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "num_k = 5\n",
    "num_sample = db_lab.shape[0]\n",
    "train_idx, test_idx = train_test_split(range(num_sample), test_size=0.15, random_state=0)\n",
    "\n",
    "kf = KFold(n_splits=num_k, shuffle=True, random_state=0)\n",
    "ktrain_idxes = []\n",
    "kval_idxes = []\n",
    "train_idx = np.array(train_idx)\n",
    "\n",
    "for ktrain_idx, kval_idx in kf.split(train_idx):\n",
    "    ktrain_idxes.append(train_idx[ktrain_idx])\n",
    "    kval_idxes.append(train_idx[kval_idx])\n",
    "    \n",
    "results_lstm = []\n",
    "for ki in range(num_k):\n",
    "    print(f'--------- k = {ki} ---------')\n",
    "    VtrainXY, VvalXY, VtestXY = data_prep(db_vital, col_vital, hr_data,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    LtrainXY, LvalXY, LtestXY = data_prep(db_lab, col_lab, hr_lab,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    mv, ml, mc, rv, rl, rc = train_val_test(VtrainXY,VvalXY,VtestXY,LtrainXY,LvalXY,LtestXY,48,0,'LSTM',0)\n",
    "    results_lstm.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'data': [results_lstm]}).to_pickle('results_lstm{}.pickle'.format(hr_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "num_k = 5\n",
    "num_sample = db_lab.shape[0]\n",
    "train_idx, test_idx = train_test_split(range(num_sample), test_size=0.15, random_state=0)\n",
    "\n",
    "kf = KFold(n_splits=num_k, shuffle=True, random_state=0)\n",
    "ktrain_idxes = []\n",
    "kval_idxes = []\n",
    "train_idx = np.array(train_idx)\n",
    "\n",
    "for ktrain_idx, kval_idx in kf.split(train_idx):\n",
    "    ktrain_idxes.append(train_idx[ktrain_idx])\n",
    "    kval_idxes.append(train_idx[kval_idx])\n",
    "    \n",
    "results_gru = []\n",
    "for ki in range(num_k):\n",
    "    print(f'--------- k = {ki} ---------')\n",
    "    VtrainXY, VvalXY, VtestXY = data_prep(db_vital, col_vital, hr_data,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    LtrainXY, LvalXY, LtestXY = data_prep(db_lab, col_lab, hr_lab,\n",
    "                                          ktrain_idxes[ki], kval_idxes[ki], test_idx,\n",
    "                                          mode='oversample', baseline=False)\n",
    "    mv, ml, mc, rv, rl, rc = train_val_test(VtrainXY,VvalXY,VtestXY,LtrainXY,LvalXY,LtestXY,48,0,'GRU',0)\n",
    "    results_gru.append(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'data': [results_gru]}).to_pickle('results_gru{}.pickle'.format(hr_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d ={'m3-baseline': [results_baseline],\n",
    "    'm3-rnn': [results_rnn],\n",
    "    'm3-lstm': [results_lstm],\n",
    "    'm3-gru': [results_gru]\n",
    "   }\n",
    "\n",
    "df= pd.DataFrame(data=d)\n",
    "df.to_pickle('result-m3.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shv = []\n",
    "for i in range(5):\n",
    "    mv = tf.keras.models.load_model(f'mv-{i}')\n",
    "    explainer = shap.DeepExplainer(mv, VtrainXY[0])\n",
    "    shap_values = explainer.shap_values(VtestXY[0])\n",
    "    shv.append(shap_values)\n",
    "\n",
    "shl = []\n",
    "for i in range(5):\n",
    "    ml = tf.keras.models.load_model(f'ml-{i}')\n",
    "    explainer = shap.DeepExplainer(ml, LtrainXY[0])\n",
    "    shap_values = explainer.shap_values(LtestXY[0])\n",
    "    shl.append(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(shv).save('m3-shv')\n",
    "np.array(shl).save('m3-shl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
